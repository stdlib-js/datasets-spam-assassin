{"id":"01675","group":"easy-ham-1","checksum":{"type":"MD5","value":"5e3a4fdad399e2557d6921d7e938faef"},"text":"Return-Path: neale@woozle.org\nDelivery-Date: Fri Sep  6 22:44:33 2002\nFrom: neale@woozle.org (Neale Pickett)\nDate: 06 Sep 2002 14:44:33 -0700\nSubject: [Spambayes] Deployment\nIn-Reply-To: <LNBBLJKPBEHFEDALKOLCGEJOBCAB.tim.one@comcast.net>\nReferences: <LNBBLJKPBEHFEDALKOLCGEJOBCAB.tim.one@comcast.net>\nMessage-ID: <w53vg5ibxoe.fsf@woozle.org>\n\nSo then, Tim Peters <tim.one@comcast.net> is all like:\n\n> [Tim]\n> > My tests train on about 7,000 msgs, and a binary pickle of the database is\n> > approaching 10 million bytes.\n> \n> That shrinks to under 2 million bytes, though, if I delete all the WordInfo\n> records with spamprob exactly equal to UNKNOWN_SPAMPROB.  Such records\n> aren't needed when scoring (an unknown word gets a made-up probability of\n> UNKNOWN_SPAMPROB).  Such records are only needed for training; I've noted\n> before that a scoring-only database can be leaner.\n\nThat's pretty good.  I wonder how much better you could do by using some\ncustom pickler.  I just checked my little dbm file and found a lot of\nwhat I would call bloat:\n\n  >>> import anydbm, hammie\n  >>> d = hammie.PersistentGrahamBayes(\"ham.db\")\n  >>> db = anydbm.open(\"ham.db\")\n  >>> db[\"neale\"], len(db[\"neale\"])\n  ('ccopy_reg\\n_reconstructor\\nq\\x01(cclassifier\\nWordInfo\\nq\\x02c__builtin__\\nobject\\nq\\x03NtRq\\x04(GA\\xce\\xbc{\\xfd\\x94\\xbboK\\x00K\\x00K\\x00G?\\xe0\\x00\\x00\\x00\\x00\\x00\\x00tb.', 106)\n  >>> d.wordinfo[\"neale\"], len(`d.wordinfo[\"neale\"]`)\n  (WordInfo'(1031337979.16197, 0, 0, 0, 0.5)', 42)\n\nIgnoring the fact that there are too many zeros in there, the pickled\nversion of that WordInfo object is over twice as large as the string\nrepresentation.  So we could get a 50% decrease in size just by using\nthe string representation instead of the pickle, right?\n\nSomething about that logic seems wrong to me, but I can't see what it\nis.  Maybe pickling is good for heterogeneous data types, but every\nvalue of our big dictionary is going to have the same type, so there's a\nton of redundancy.  I guess that explains why it compressed so well.\n\nNeale\n"}