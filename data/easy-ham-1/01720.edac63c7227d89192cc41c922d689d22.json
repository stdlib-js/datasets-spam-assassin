{"id":"01720","group":"easy-ham-1","checksum":{"type":"MD5","value":"edac63c7227d89192cc41c922d689d22"},"text":"Return-Path: tim.one@comcast.net\nDelivery-Date: Sun Sep  8 20:48:13 2002\nFrom: tim.one@comcast.net (Tim Peters)\nDate: Sun, 08 Sep 2002 15:48:13 -0400\nSubject: [Spambayes] testing results\nIn-Reply-To: <20020908172113.GA26741@glacier.arctrix.com>\nMessage-ID: <LNBBLJKPBEHFEDALKOLCCEPKBCAB.tim.one@comcast.net>\n\nNeil trained a classifier using 3 sets with about 500 ham and spam in each.\nWe're missing half his test run results due to a cmp.py bug (since fixed);\nthe \"before custom fiddling\" figures on the 3 reported runs were:\n\n    false positive percentages\n        0.187\n        0.749\n        0.780\n    total unique fp 19\n\n    false negative percentages\n        2.072\n        2.448\n        0.574\n    total unique fn 43\n\nThe \"total unique\" figures counts all 6 runs; it's just the individual-run\nfp and fn percentages we're missing for 3 runs.\n\nJeremy reported these \"before custom fiddling\" figures on 4 sets with about\n600 ham and spam in each:\n\n    false positive percentages\n        0.000\n        1.398\n        1.398\n        0.000\n        1.242\n        1.242\n        1.398\n        1.398\n        0.000\n        1.553\n        1.553\n        0.000\n    total unique fp 139\n\n    false negative percentages\n       10.413\n        6.104\n        5.027\n        8.259\n        2.873\n        5.745\n        5.206\n        4.488\n        9.336\n        5.206\n        5.027\n        9.874\n    total unique fn 970\n\nSo things are clearly working much better for Neil.  Both reported\nsignificant improvements in both f-n and f-p rates by folding in more header\nlines.  Neal added Received analysis to the base tokenizer's header\nanalysis, and Jeremy skipped the base tokenizer's header analysis completely\nbut added base-subject-line-like but case-folded tokenization for almost all\nheader lines (excepting only Received, Data, X-From_, and, I *suspect*, all\nthose starting with 'x-vm').\n\nWhen I try 5 random pairs of 500-ham + 500-spam subsets in my test data, I\nsee:\n\n    false positive percentages\n        0.000\n        0.000\n        0.200\n        0.000\n        0.200\n        0.000\n        0.200\n        0.000\n        0.000\n        0.200\n        0.400\n        0.000\n        0.200\n        0.000\n        0.200\n        0.400\n        0.000\n        0.400\n        0.200\n        0.600\n    total unique fp 10\n\n    false negative percentages\n        0.800\n        0.400\n        0.200\n        0.600\n        1.000\n        0.000\n        0.600\n        1.200\n        1.200\n        0.800\n        0.400\n        0.800\n        1.800\n        0.800\n        0.400\n        1.000\n        1.000\n        0.400\n        0.000\n        0.600\n    total unique fn 36\n\nThis is much closer to what Neil saw, but still looks better.  Another run\non a disjoint 5 random pairs looked much the same; total unique fp rose to\n12 and fn fell to 27; on a third run with another set of disjoint 5 random\npairs, likewise, with fp 12 and fn 40.  So I'm pretty confident that it's\nnot going to matter which random subsets of 500 I take from my data.\n\nIt's hard to conclude anything given Jeremy's much worse results.  If they\nwere in line with Neil's results, I'd suspect that I've over-tuned the\nalgorithm to statistical quirks in my corpora.\n\n"}