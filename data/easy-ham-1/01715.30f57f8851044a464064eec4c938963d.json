{"id":"01715","group":"easy-ham-1","checksum":{"type":"MD5","value":"30f57f8851044a464064eec4c938963d"},"text":"Return-Path: tim.one@comcast.net\nDelivery-Date: Sun Sep  8 08:18:49 2002\nFrom: tim.one@comcast.net (Tim Peters)\nDate: Sun, 08 Sep 2002 03:18:49 -0400\nSubject: [Spambayes] test sets?\nIn-Reply-To: <200209080538.g885cjk17553@pcp02138704pcs.reston01.va.comcast.net>\nMessage-ID: <LNBBLJKPBEHFEDALKOLCGEOIBCAB.tim.one@comcast.net>\n\n[Guido]\n> I *meant* to say that they were 0.99 clues cancelled out by 0.01\n> clues.  But that's wrong too!  It looks I haven't grokked this part of\n> your code yet; this one has way more than 16 clues, and it seems the\n> classifier basically ended up counting way more 0.99 than 0.01 clues,\n> and no others made it into the list.  I thought it was looking for\n> clues with values in between; apparently it found none that weren't\n> exactly 0.5?\n\nThere's a brief discussion of this before the definition of\nMAX_DISCRIMINATORS.  All clues with prob MIN_SPAMPROB and MAX_SPAMPROB are\nsaved in min and max lists, and all other clues are fed into the nbest heap.\nThen the shorter of the min and max lists cancels out the same number of\nclues in the longer list.  Whatever remains of the longer list (if anything)\nis then fed into the nbest heap too, but no more than MAX_DISCRIMINATORS of\nthem.  In no case do more than MAX_DISCRIMINATORS clues enter into the final\nprobability calculation, but all of the min and max lists go into the list\nof clues (else you'd have no clue that massive cancellation was occurring;\nand massive cancellation may yet turn out to be a hook to signal that manual\nreview is needed).  In your specific case, the excess of clues in the longer\nMAX_SPAMPROB list pushed everything else out of the nbest heap, and that's\nwhy you didn't see anything other than 0.01 and 0.99.\n\nBefore adding these special lists, the outcome when faced with many 0.01 and\n0.99 clues was too often a coin toss:  whichever flavor just happened to\nappear MAX_DISCRIMINATORS//2 + 1 times first determined the final outcome.\n\n>> That sure sets the record for longest list of cancelling extreme clues!\n\n> This happened to be the longest one, but there were quite a few\n> similar ones.\n\nI just beat it <wink>:  a tokenization scheme that folds case, and ignores\npunctuation, and strips a trailing 's' from words, and saves both word\nbigrams and word unigrams, turned up a low-probability very long spam with a\nlist of 410 0.01 clues and 125 0.99 clues!  Yikes.\n\n> I wonder if there's anything we can learn from looking at the clues and\nthe\n> HTML.  It was heavily marked-up HTML, with ads in the sidebar, but the\nbody\n> text was a serious discussion of \"OO and soft coding\" with lots of highly\n> technical words as clues (including Zope and ZEO).\n\nNo matter how often it says Zope, it gets only one 0.01 clue from doing so.\nDitto for ZEO.  In contrast, HTML markup has many unique \"words\" that get\n0.99.  BTW, this is a clear case where the assumption of\nconditionally-independent word probabilities is utterly bogus -- e.g., the\nprobability that \"<body>\" appears in a message is highly correlated with the\nprob of \"<br>\" appearing.  By treating them as independent, naive Bayes\ngrossly misjudges the probability that both appear, and the only thing you\nget in return is something that can actually be computed <wink>.\n\nRead the \"What about HTML?\" section in tokenizer.py.  From the very start,\nI've been investigating what would work best for the mailing lists hosted at\npython.org, and HTML decorations have so far been too strong a clue to\njustify ignoring it in that specific context.  I haven't done anything\ngeared toward personal email, including the case of non-mailing-list email\nthat happens to go through python.org.\n\nI'd prefer to strip HTML tags from everything, but last time I tried that it\nstill had bad effects on the error rates in my corpora (the full test\nresults with and without HTML tag stripping is included in the \"What about\nHTML?\" comment block).  But as the comment block also says,\n\n# XXX So, if another way is found to slash the f-n rate, the decision here\n# XXX not to strip HTML from HTML-only msgs should be revisited.\n\nand we've since done several things that gave significant f-n rate\nreductions.  I should test that again now.\n\n> Are there any minable-but-unmined header lines in your corpus left?\n\nAlmost all of them -- apart from MIME decorations that appear in both\nheaders and bodies (like Content-Type), the *only* header lines the base\ntokenizer looks at now are Subject, From, X-Mailer, and Organization.\n\n> Or do we have to start with a different corpus before we can make\n> progress there?\n\nI would need different data, yes.  My ham is too polluted with Mailman\nheader decorations (which I may or may not be able to clean out, but fudging\nthe data is a Mortal Sin and I haven't changed a byte so far), and my spam\ntoo polluted with header clues about the fellow who collected it.  In\nparticular I have to skip To and Received headers now, and I suspect they're\ngoing to be very valuable in real life (for example, I don't even catch\n\"undisclosed recipients\" in the To header now!).\n\n> ...\n> No, sorry.  These were all of the following structure:\n>\n>   multipart/mixed\n>       text/plain        (brief text plus URL(s))\n>       text/html         (long HTML copied from website)\n\nAh!  That explains why the HTML tags didn't get stripped.  I'd again offer\nto add an optional argument to tokenize() so that they'd get stripped here\ntoo, but if it gets glossed over a third time that would feel too much like\na loss <wink>.\n\n>> This seems confused: Jeremy didn't use my trained classifier pickle,\n>> he trained his own classifier from scratch on his own corpora.\n>> ...\n\n> I think it's still corpus size.\n\nI reported on tests I ran with random samples of 220 spams and 220 hams from\nmy corpus (that means training on sets of those sizes as well as predicting\non sets of those sizes), and while that did harm the error rates, the error\nrates I saw were still much better than Jeremy reported when using 500 of\neach.\n\n\nAh, a full test run just finished, on the\n\n   tokenization scheme that folds case, and ignores punctuation, and strips\na\n   trailing 's' from words, and saves both word bigrams and word unigrams\n\nThis is the code:\n\n            # Tokenize everything in the body.\n            lastw = ''\n            for w in word_re.findall(text):\n                n = len(w)\n                # Make sure this range matches in tokenize_word().\n                if 3 <= n <= 12:\n                    if w[-1] == 's':\n                        w = w[:-1]\n                    yield w\n                    if lastw:\n                        yield lastw + w\n                    lastw = w + ' '\n\n                elif n >= 3:\n                    lastw = ''\n                    for t in tokenize_word(w):\n                        yield t\n\nwhere\n\n    word_re = re.compile(r\"[\\w$\\-\\x80-\\xff]+\")\n\nThis at least doubled the process size over what's done now.  It helped the\nf-n rate significantly, but probably hurt the f-p rate (the f-p rate is too\nlow with only 4000 hams per run to be confident about changes of such small\n*absolute* magnitude -- 0.025% is a single message in the f-p table):\n\nfalse positive percentages\n    0.000  0.000  tied\n    0.000  0.075  lost  +(was 0)\n    0.050  0.125  lost  +150.00%\n    0.025  0.000  won   -100.00%\n    0.075  0.025  won    -66.67%\n    0.000  0.050  lost  +(was 0)\n    0.100  0.175  lost   +75.00%\n    0.050  0.050  tied\n    0.025  0.050  lost  +100.00%\n    0.025  0.000  won   -100.00%\n    0.050  0.125  lost  +150.00%\n    0.050  0.025  won    -50.00%\n    0.050  0.050  tied\n    0.000  0.025  lost  +(was 0)\n    0.000  0.025  lost  +(was 0)\n    0.075  0.050  won    -33.33%\n    0.025  0.050  lost  +100.00%\n    0.000  0.000  tied\n    0.025  0.100  lost  +300.00%\n    0.050  0.150  lost  +200.00%\n\nwon   5 times\ntied  4 times\nlost 11 times\n\ntotal unique fp went from 13 to 21\n\nfalse negative percentages\n    0.327  0.218  won    -33.33%\n    0.400  0.218  won    -45.50%\n    0.327  0.218  won    -33.33%\n    0.691  0.691  tied\n    0.545  0.327  won    -40.00%\n    0.291  0.218  won    -25.09%\n    0.218  0.291  lost   +33.49%\n    0.654  0.473  won    -27.68%\n    0.364  0.327  won    -10.16%\n    0.291  0.182  won    -37.46%\n    0.327  0.254  won    -22.32%\n    0.691  0.509  won    -26.34%\n    0.582  0.473  won    -18.73%\n    0.291  0.255  won    -12.37%\n    0.364  0.218  won    -40.11%\n    0.436  0.327  won    -25.00%\n    0.436  0.473  lost    +8.49%\n    0.218  0.218  tied\n    0.291  0.255  won    -12.37%\n    0.254  0.364  lost   +43.31%\n\nwon  15 times\ntied  2 times\nlost  3 times\n\ntotal unique fn went from 106 to 94\n\n"}