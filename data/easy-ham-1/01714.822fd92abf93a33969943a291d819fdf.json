{"id":"01714","group":"easy-ham-1","checksum":{"type":"MD5","value":"822fd92abf93a33969943a291d819fdf"},"text":"Return-Path: guido@python.org\nDelivery-Date: Sun Sep  8 06:38:45 2002\nFrom: guido@python.org (Guido van Rossum)\nDate: Sun, 08 Sep 2002 01:38:45 -0400\nSubject: [Spambayes] test sets?\nIn-Reply-To: Your message of \"Sun, 08 Sep 2002 01:02:32 EDT.\"\n             <LNBBLJKPBEHFEDALKOLCEEOGBCAB.tim.one@comcast.net> \nReferences: <LNBBLJKPBEHFEDALKOLCEEOGBCAB.tim.one@comcast.net> \nMessage-ID: <200209080538.g885cjk17553@pcp02138704pcs.reston01.va.comcast.net>\n\n> > I also looked in more detail at some f-p's in my geeks traffic.  The\n> > first one's a doozie (that's the term, right? :-).  It has lots of\n> > HTML clues that are apparently ignored.\n> \n> ?? The clues below are *loaded* with snippets unique to HTML (like '<br>').\n\nI *meant* to say that they were 0.99 clues cancelled out by 0.01\nclues.  But that's wrong too!  It looks I haven't grokked this part of\nyour code yet; this one has way more than 16 clues, and it seems the\nclassifier basically ended up counting way more 0.99 than 0.01 clues,\nand no others made it into the list.  I thought it was looking for\nclues with values in between; apparently it found none that weren't\nexactly 0.5?\n\n> That sure sets the record for longest list of cancelling extreme clues!\n\nThis happened to be the longest one, but there were quite a few\nsimilar ones.  I wonder if there's anything we can learn from looking\nat the clues and the HTML.  It was heavily marked-up HTML, with ads in\nthe sidebar, but the body text was a serious discussion of \"OO and\nsoft coding\" with lots of highly technical words as clues (including\nZope and ZEO).\n\n> That there are *any* 0.50 clues in here means the scheme ran out of\n> anything interesting to look at.  Adding in more header lines should\n> cure that.\n\nAre there any minable-but-unmined header lines in your corpus left?\nOr do we have to start with a different corpus before we can make\nprogress there?\n\n> > The seventh was similar.\n> >\n> > I scanned a bunch more until I got bored, and most of them were either\n> > of the first form (brief text with URL followed by quoted HTML from\n> > website)\n> \n> If those were text/plain, the HTML tags should have been stripped.  I'm\n> still confused about this part.\n\nNo, sorry.  These were all of the following structure:\n\n  multipart/mixed\n      text/plain        (brief text plus URL(s))\n      text/html         (long HTML copied from website)\n\nI guess you get this when you click on \"mail this page\" in some\nbrowsers.\n\n> That HTML tags aren't getting stripped remains the biggest mystery to me.\n\nStill?\n\n> This seems confused: Jeremy didn't use my trained classifier pickle,\n> he trained his own classifier from scratch on his own corpora.\n> That's an entirely different kind of experiment from the one you're\n> trying (indeed, you're the only one so far to report results from\n> trying my pickle on their own email, and I never expected *that* to\n> work well; it's a much bigger mystery to me why Jeremy got such\n> relatively worse results from training his own -- and he's the only\n> one so far to report results from *that* experiment).\n\nI think it's still corpus size.\n\n--Guido van Rossum (home page: http://www.python.org/~guido/)\n"}