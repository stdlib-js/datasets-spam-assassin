{"id":"01705","group":"easy-ham-1","checksum":{"type":"MD5","value":"54f25b5c3ce8b81c56da0ef6693c5ffb"},"text":"Return-Path: tim.one@comcast.net\nDelivery-Date: Sat Sep  7 20:45:18 2002\nFrom: tim.one@comcast.net (Tim Peters)\nDate: Sat, 07 Sep 2002 15:45:18 -0400\nSubject: [Spambayes] understanding high false negative rate\nIn-Reply-To: <15738.13529.407748.635725@slothrop.zope.com>\nMessage-ID: <LNBBLJKPBEHFEDALKOLCOENBBCAB.tim.one@comcast.net>\n\n>   TP> I'm reading this now as that you trained on about 220 spam and\n>   TP> about 220 ham.  That's less than 10% of the sizes of the\n>   TP> training sets I've been using.  Please try an experiment: train\n>   TP> on 550 of each, and test once against the other 550 of each.\n\n[Jeremy]\n> This helps a lot!\n\nPossibly.  I checked in a change to classifier.py overnight (getting rid of\nMINCOUNT) that gave a major improvement in the f-n rate too, independent of\ntokenization.\n\n> Here are results with the stock tokenizer:\n\nUnsure what \"stock tokenizer\" means to you.  For example, it might mean\ntokenizer.tokenize, or mboxtest.MyTokenizer.tokenize.\n\n> Training on <mbox: /home/jeremy/Mail/INBOX 0> & <mbox:\n> /home/jeremy/Mail/spam 8>\n>  ... 644 hams & 557 spams\n>       0.000  10.413\n>       1.398   6.104\n>       1.398   5.027\n> Training on <mbox: /home/jeremy/Mail/INBOX 0> & <mbox:\n> /home/jeremy/Mail/spam 0>\n>  ... 644 hams & 557 spams\n>       0.000   8.259\n>       1.242   2.873\n>       1.242   5.745\n> Training on <mbox: /home/jeremy/Mail/INBOX 2> & <mbox:\n> /home/jeremy/Mail/spam 3>\n>  ... 644 hams & 557 spams\n>       1.398   5.206\n>       1.398   4.488\n>       0.000   9.336\n> Training on <mbox: /home/jeremy/Mail/INBOX 2> & <mbox:\n> /home/jeremy/Mail/spam 0>\n>  ... 644 hams & 557 spams\n>       1.553   5.206\n>       1.553   5.027\n>       0.000   9.874\n> total false pos 139 5.39596273292\n> total false neg 970 43.5368043088\n\nNote that those rates remain much higher than I got using just 220 of ham\nand 220 of spam.  That remains A Mystery.\n\n> And results from the tokenizer that looks at all headers except Date,\n> Received, and X-From_:\n\nUnsure what that means too.  For example, \"looks at\" might mean you enabled\nAnthony's count-them gimmick, and/or that you're tokenizing them yourself,\nand/or ...?\n\n> Training on <mbox: /home/jeremy/Mail/INBOX 0> & <mbox:\n> /home/jeremy/Mail/spam 8>\n>  ... 644 hams & 557 spams\n>       0.000   7.540\n>       0.932   4.847\n>       0.932   3.232\n> Training on <mbox: /home/jeremy/Mail/INBOX 0> & <mbox:\n> /home/jeremy/Mail/spam 0>\n>  ... 644 hams & 557 spams\n>       0.000   7.181\n>       0.621   2.873\n>       0.621   4.847\n> Training on <mbox: /home/jeremy/Mail/INBOX 2> & <mbox:\n> /home/jeremy/Mail/spam 3>\n>  ... 644 hams & 557 spams\n>       1.087   4.129\n>       1.087   3.052\n>       0.000   6.822\n> Training on <mbox: /home/jeremy/Mail/INBOX 2> & <mbox:\n> /home/jeremy/Mail/spam 0>\n>  ... 644 hams & 557 spams\n>       0.776   3.411\n>       0.776   3.411\n>       0.000   6.463\n> total false pos 97 3.76552795031\n> total false neg 738 33.1238779174\n>\n> Is it safe to conclude that avoiding any cleverness with headers is a\n> good thing?\n\nSince I don't know what you did, exactly, I can't guess.  What you seemed to\nshow is that you did *something* clever with headers and that doing so\nhelped (the \"after\" numbers are better than the \"before\" numbers, right?).\nAssuming that what you did was override what's now\ntokenizer.Tokenizer.tokenize_headers with some other routine, and didn't\ncall the base Tokenizer.tokenize_headers at all, then you're missing\ncarefully tested treatment of just a few header fields, but adding many\ndozens of other header fields.  There's no question that adding more header\nfields should help; tokenizer.Tokenizer.tokenize_headers doesn't do so only\nbecause my testing corpora are such that I can't add more headers without\ngetting major benefits for bogus reasons.\n\nApart from all that, you said you're skipping Received.  By several\naccounts, that may be the most valuable of all the header fields.  I'm\n(meaning tokenizer.Tokenizer.tokenize_headers) skipping them too for the\nreason explained above.  Offline a week or two ago, Neil Schemenauer\nreported good results from this scheme:\n\n    ip_re = re.compile(r'(\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3})')\n\n    for header in msg.get_all(\"received\", ()):\n        for ip in ip_re.findall(header):\n            parts = ip.split(\".\")\n            for n in range(1, 5):\n                yield 'received:' + '.'.join(parts[:n])\n\nThis makes a lot of sense to me; I just checked it in, but left it disabled\nfor now.\n\n"}